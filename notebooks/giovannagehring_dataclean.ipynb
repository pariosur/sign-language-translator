{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd52cc4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53569d45",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports\n",
    "\n",
    "Packages that need to be imported:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9965be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c9f46",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read dataframe\n",
    "\n",
    "Read json file with words and the list of videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec5e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/dxli94/WLASL/master/start_kit/WLASL_v0.3.json\"\n",
    "df = pd.read_json(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bac149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export a list of words in csv\n",
    "#df[\"gloss\"].to_csv(\"list_words.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34a0dd9",
   "metadata": {},
   "source": [
    "### Filter df only selected words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c543548",
   "metadata": {},
   "source": [
    "Filter the json file with words that will be used to train our model. This file will be used later to download the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb1ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_selected_words = [\"headache\", \"cough\", \"sore throat\", \"blood\", \"pregnant\", \"diabetes\", \n",
    "                       \"stomach\",\"pain\", \"allergy\", \"cold\", \"bone\",\"diarrhea\", \"heart\", \"heart attack\", \n",
    "                       \"cochlear implant\", \"vomit\", \"depressed\", \"hurt\", \"infection\", \"tired\", \"thank you\"]\n",
    "\n",
    "df = df[df['gloss'].isin(list_selected_words)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Df shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21de09ed",
   "metadata": {},
   "source": [
    "### Export pandas df to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a0cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_json = df.to_json(\"WLASL_v0.3.json\", orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c911748c",
   "metadata": {},
   "source": [
    "### Explore instances\n",
    "\n",
    "Create a dataframe with video_ids and the corresponding word. Notice that in this step we are using the json file exported in the previous step, which contains information about only 21 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303c95df",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = json.load(open(\"../scripts/WLASL_v0.3.json\"))\n",
    "\n",
    "id_videos = []\n",
    "for entry in content:\n",
    "    word = entry[\"gloss\"]\n",
    "    instances = entry['instances']\n",
    "\n",
    "    for inst in instances:\n",
    "        video_id = inst['video_id']\n",
    "        id_videos.append([word, video_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408351a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id_list = pd.DataFrame(id_videos, columns=[\"word\", \"video_id\"])\n",
    "id_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e839dd32",
   "metadata": {},
   "source": [
    "Because of problems during download, we were not able to download all video_ids. To count the real number of videos that we have available, we need to check if they are in our folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f096e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_in_folder = []\n",
    "\n",
    "for video_id in id_list.video_id:\n",
    "    if os.path.exists(f'../raw_data/{video_id}.mp4'):\n",
    "        ids_in_folder.append(video_id)\n",
    "\n",
    "print(f\"Number os videos: {len(ids_in_folder)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6361477b",
   "metadata": {},
   "source": [
    "Filter dataframe to only videos available in our folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f3f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = id_list[id_list['video_id'].isin(ids_in_folder)]\n",
    "print(f\"Shape new df: {id_list.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29da871",
   "metadata": {},
   "source": [
    "#### Count videos per symptoms\n",
    "Check how many videos we have, by symptom, to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777bc7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_list = id_list.groupby(\"word\").count().reset_index()\n",
    "aux_list.sort_values(\"video_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85311f2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf61e2",
   "metadata": {},
   "source": [
    "### Setup Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0dc6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('../MP_Data') \n",
    "\n",
    "# Videos are going to be 70 frames in length\n",
    "sequence_length = 70 #wont be used anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac548d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Loop to create folders for each action to be trained\n",
    "# and n_folder depending on the number of videos\n",
    "# ----------------------------------------------------\n",
    "\n",
    "for word in range(0, len(aux_list.word)):   # modificar range depois dos testes (0, len..)\n",
    "    \n",
    "    # Identify how many videos per action\n",
    "    n_folders = aux_list.iloc[word, 1]\n",
    "    \n",
    "    # Create folders\n",
    "    for n in range(n_folders):\n",
    "        try:\n",
    "            folder = os.path.join(DATA_PATH, aux_list.iloc[word, 0], str(n))\n",
    "            os.makedirs(folder)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b2f741",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions for data detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df4283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af1ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fa2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mediapipe_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14d98f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b88f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9449099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e952f",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "## Length Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4efb9d6",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "### To check lenght of videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6070511e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function to extract video id for selected word - Giovanna \n",
    "def videos_word(word):\n",
    "  \n",
    "    id_list_novo = id_list[id_list['word'] == word]\n",
    "    lista_videos = [video_id for video_id in id_list_novo.video_id]\n",
    "    \n",
    "    return lista_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175eed33",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function to return duration and framecount for each video\n",
    "def with_opencv(filename):\n",
    "    import cv2\n",
    "    video = cv2.VideoCapture(filename)\n",
    "\n",
    "    duration = video.get(cv2.CAP_PROP_POS_MSEC)\n",
    "    frame_count = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "    return duration, frame_count\n",
    "\n",
    "\n",
    "\n",
    "# Get id_videos from function video_words and return lenght\n",
    "n_frames = []\n",
    "teste = []\n",
    "\n",
    "for word in aux_list[\"word\"]:\n",
    "    for video_id in videos_word(word):\n",
    "        video = f'../raw_data/{video_id}.mp4'\n",
    "        frame_count = with_opencv(video)[1]\n",
    "        n_frames.append(frame_count)\n",
    "        teste.append([word, video_id, frame_count])\n",
    "        \n",
    "frames_words = pd.DataFrame(teste, columns=[\"action\", \"video_id\", \"frames\"])\n",
    "frames_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0968f4a1-5394-404e-829e-28285deeeae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution frames \n",
    "plt.title(\"Distribution of Frames\")\n",
    "plt.xlabel('Frames')\n",
    "plt.ylabel('Frequency')\n",
    "plt.hist(pd.DataFrame(n_frames), bins=30, color=\"g\", stacked=True);\n",
    "plt.text(80, 15, f'Optimal number \\n of frames=70');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92255e05-69eb-4587-90b1-ff397dc0d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve words, video, frames\n",
    "\n",
    "def words_video(word):\n",
    "    w = frames_words[frames_words['action'] == word].reset_index()\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cf6a91-e825-4fd4-afbe-ccf164debf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_video(\"cochlear implant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a56099-ae3a-43da-a532-e661b7aafa10",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Points - New"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b504e7e9-8da9-4b85-8178-1dc83895f743",
   "metadata": {},
   "source": [
    "#### Model Plan A - with loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df1d80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Function to extract data points\n",
    "# Action = word\n",
    "# Folder = /word/number_folder\n",
    "# Video_id = name of the video\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "def extract_datapoints(action, folder, video_id, frames):\n",
    "    \n",
    "    data_path = os.path.join('../MP_Data')\n",
    "    \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "        for frame_num in range(frames):\n",
    "\n",
    "                    # Read feed\n",
    "                    ret, frame = cap.read()\n",
    "\n",
    "                    # Make detections\n",
    "                    try:\n",
    "                        image, results = mediapipe_detection(frame, holistic)\n",
    "                        print(results)\n",
    "\n",
    "                        # Draw landmarks and text\n",
    "                        draw_styled_landmarks(image, results)\n",
    "                        cv2.putText(image, 'Frames for {} video {}'.format(action, folder), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                        # Show to screen\n",
    "                        cv2.imshow('OpenCV Feed', image)\n",
    "                        cv2.waitKey(500)\n",
    "\n",
    "                        # Export keypoints\n",
    "                        keypoints = extract_keypoints(results)\n",
    "                        npy_path = os.path.join(data_path, action, folder, str(frame_num))\n",
    "                        np.save(npy_path, keypoints)\n",
    "\n",
    "                        # Break gracefully\n",
    "                        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                            break\n",
    "                    except:\n",
    "                        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26034625-57fb-41a1-9086-aaab8bb9b61f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Loop to extract data points using function\n",
    "# ---------------------------------------------------\n",
    "\n",
    "selected_word = [\"pregnant\"]\n",
    "\n",
    "for word in selected_word:\n",
    "            \n",
    "    # Filter the dataframe\n",
    "    df_temp = frames_words[frames_words[\"action\"] == word] \n",
    "\n",
    "    # Get information for each action\n",
    "    for word in range(0, len(df_temp.action)):\n",
    "\n",
    "        action = df_temp.iloc[word, 0]\n",
    "        cap = cv2.VideoCapture(f\"../raw_data/{df_temp.iloc[word, 1]}.mp4\")\n",
    "        sequence_folder = word\n",
    "        frames = [int(df_temp.iloc[word, 2]) if int(df_temp.iloc[word, 2]) < 70 else 70][0]\n",
    "\n",
    "        # Call the function to extract data points\n",
    "        extract_datapoints(action, str(sequence_folder), cap, frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b51eae-ecac-4dfd-8ece-204f30d45481",
   "metadata": {},
   "source": [
    "#### Model  - Plan B "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4362280f-e42c-442a-b9f3-4f58d6ee105d",
   "metadata": {
    "tags": []
   },
   "source": [
    "cap = cv2.VideoCapture(f'../raw_data/40807.mp4')\n",
    "ret, frame = cap.read()\n",
    "\n",
    "ret"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2683249c-f804-41d4-abd8-a6c50202a985",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set mediapipe model \n",
    "\n",
    "cap = cv2.VideoCapture(f'../raw_data/40807.mp4') # Video exemplo muito curto - 50 frames dÃ¡ erro!\n",
    "\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    data_path = os.path.join('../MP_Data')\n",
    "    \n",
    "    for frame_num in range(22):\n",
    "\n",
    "                # Read feed\n",
    "            \n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                print(results)\n",
    "                \n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(\"teste\", 1), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 0, 255), 1, cv2.LINE_AA)\n",
    "                \n",
    "                 # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                cv2.waitKey(500)\n",
    "                \n",
    "                \n",
    "                # Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(data_path, \"teste\", str(1), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4817d7-2663-409b-87fe-80879db71a72",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Np array comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b4cedd-3799-4f3d-9ebc-b7784a1a1460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sem loop\n",
    "for i in range(0, 25):\n",
    "    print(np.load(f\"../MP_Data/stomach/4/{i}.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc413cab-c9ac-40ad-bf02-1d45df25b2a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Com loop\n",
    "for i in range(5, 15):\n",
    "    print(np.load(f\"../MP_Data/teste/1/{i}.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861476eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Extract Data Points - Old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885387f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract data points from videos"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f92a00f9-d9f2-4c72-9f4d-3c37751ffaeb",
   "metadata": {},
   "source": [
    "#extract_data_points(\"tired\", \"01960\", 0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3aa4b4fd-6a63-499f-881a-ad7fdf99d011",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------\n",
    "# Function to extract data points from videos\n",
    "# Action = words\n",
    "# Video_id = name of the video\n",
    "# Sequence = folder in which it will be placed\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def extract_data_points(action, video_id, folder):\n",
    "    \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "        for frame_num in range(sequence_length):\n",
    "            \n",
    "            sequence = 1\n",
    "\n",
    "            # Read feed\n",
    "            cap = cv2.VideoCapture(f'../raw_data/{video_id}.mp4')\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            print(results)\n",
    "\n",
    "            # Draw landmarks\n",
    "            draw_styled_landmarks(image, results)\n",
    "\n",
    "            # Apply wait logic\n",
    "            if frame_num == 0: \n",
    "                cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                cv2.waitKey(2000)\n",
    "            else: \n",
    "                cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "            # Export keypoints\n",
    "            keypoints = extract_keypoints(results)\n",
    "            npy_path = os.path.join(DATA_PATH, action, str(folder), str(frame_num))\n",
    "            np.save(npy_path, keypoints)\n",
    "\n",
    "            # Break gracefully\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "                    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "36479b17-c1d3-4bb9-8d8e-7ea9cae42cb6",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------\n",
    "# Loop to extract data points for each word and video\n",
    "# and save it in the correct folder\n",
    "# ---------------------------------------------------\n",
    "\n",
    "for word in id_list.word.unique():\n",
    "    \n",
    "    if word == \"tired\": #apagar depois dos testes, filtra apenas 1 palavra\n",
    "        \n",
    "        # Filter the dataframe\n",
    "        df_temp = id_list[id_list[\"word\"] == word] \n",
    "\n",
    "        # Get information for each action\n",
    "        for word in range(0, len(df_temp.word)):\n",
    "\n",
    "            action = df_temp.iloc[word, 0]\n",
    "            video_id = df_temp.iloc[word, 1]\n",
    "            sequence = word\n",
    "\n",
    "            # Call the function to extract data points\n",
    "            extract_data_points(action, video_id, sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179848be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract data points from webcam"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bbadaec-7f76-4ce7-ace4-643527a5592e",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------\n",
    "# Function to extract data points from webcam to\n",
    "# complement actions with less than x videos. This\n",
    "# function also creates additional folders\n",
    "\n",
    "# Action = list of words\n",
    "# no_sequence = number of additional videos to create\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def extract_data_points_webcam(actions, no_sequences):\n",
    "    \n",
    "    # Extract video from webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Set mediapipe model \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "        # Loop through actions\n",
    "        for action in actions:\n",
    "            \n",
    "            #Identify number of folders for this action\n",
    "            start_folder = int(max(os.listdir(f\"../MP_Data/{action}\")))+1\n",
    "            \n",
    "            # Loop through sequences aka videos\n",
    "            for sequence in range(start_folder, start_folder+no_sequences):\n",
    "                \n",
    "                # Create folder before export\n",
    "                os.makedirs(os.path.join(DATA_PATH, action, str(sequence))) \n",
    "                \n",
    "                # Loop through video length aka sequence length\n",
    "                for frame_num in range(sequence_length):\n",
    "\n",
    "                    # Read feed\n",
    "                    ret, frame = cap.read()\n",
    "\n",
    "                    # Make detections\n",
    "                    image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                    # Draw landmarks\n",
    "                    draw_styled_landmarks(image, results)\n",
    "\n",
    "                    # Apply wait logic\n",
    "                    if frame_num == 0: \n",
    "                        cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                        cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                        # Show to screen\n",
    "                        cv2.imshow('OpenCV Feed', image)\n",
    "                        cv2.waitKey(500)\n",
    "                    else: \n",
    "                        cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                        # Show to screen\n",
    "                        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                    # Export keypoints\n",
    "                    keypoints = extract_keypoints(results)                    \n",
    "                    npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                    np.save(npy_path, keypoints)\n",
    "\n",
    "                    # Break gracefully\n",
    "                    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                        break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "                    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fdbd7a8-4384-4131-9cc5-38a41aac10f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ---------------------------------------------------\n",
    "# Create additional videos for selected words\n",
    "# ---------------------------------------------------\n",
    "\n",
    "list_words = [\"vomit\"]\n",
    "extract_data_points_webcam(list_words, 5)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3058c1b0-06a0-4ed6-9113-ec778cbcef4e",
   "metadata": {},
   "source": [
    "for word in id_list.word.unique():\n",
    "    \n",
    "    if word == \"tired\": #apagar depois dos testes, filtra apenas 1 palavra\n",
    "        \n",
    "        # Filter the dataframe\n",
    "        df_temp = id_list[id_list[\"word\"] == word] \n",
    "\n",
    "        # Get information for each action\n",
    "        for word in range(0, len(df_temp.word)):\n",
    "\n",
    "            action = df_temp.iloc[word, 0]\n",
    "            video_id = df_temp.iloc[word, 1]\n",
    "            sequence = word\n",
    "\n",
    "            # Call the function to extract data points\n",
    "            print(action, video_id, sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5dd0ff-9819-46d8-a90d-ecde1a6b0986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "81bba0a3e1aea3246bccf1314a02fd945e9b390bb368e98c40a6cfa7697ea8c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
